{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning pipeline Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import preprocessed sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_prepro = pd.read_csv('../data/sample/corona_sample.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the data with the following steps (in sequence):\n",
    "- Remove username\n",
    "- Remove emoticon\n",
    "- Remove url\n",
    "- Remove html\n",
    "- Remove stopwords\n",
    "- Perform lemmatisation\n",
    "- Remove unknown words (including acronyms)\n",
    "- Remove tweets that are shorter than minimum length\n",
    "- Remove duplicated rows (based on the tweet text)\n",
    "- Convert the created_date to date_time format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_username(entry):\n",
    "    pattern = r'@.+?\\s'\n",
    "    output = re.sub(pattern, '', entry).strip()\n",
    "    return output\n",
    "\n",
    "df_prepro['tweet'] = df_prepro.tweet.apply(remove_username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoticon(inputString):\n",
    "    return inputString.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "df_prepro['tweet'] = df_prepro.tweet.apply(remove_emoticon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(entry):\n",
    "    pattern = r'http\\S+'\n",
    "    output = re.sub(pattern, '', entry).strip()\n",
    "    return output\n",
    "\n",
    "df_prepro['tweet'] = df_prepro.tweet.apply(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(entry):\n",
    "    pattern = r'<.+?>'\n",
    "    output = re.sub(pattern, '', entry).strip()\n",
    "    return output\n",
    "\n",
    "df_prepro['tweet'] = df_prepro.tweet.apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_word = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_word(entry):    \n",
    "    output = [w for w in word_tokenize(entry) if w.lower() not in stop_word]\n",
    "    output = ' '.join(output)\n",
    "    return output\n",
    "\n",
    "df_prepro['tweet'] = df_prepro.tweet.apply(remove_stop_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmat(entry):\n",
    "    tokens = [word for word in word_tokenize(entry.lower()) if (word.isalpha() or word.isnumeric())]\n",
    "    tags = nltk.pos_tag_sents([tokens])\n",
    "    output = []\n",
    "    for i,tk in enumerate(tokens):\n",
    "        tag = tags[0][i][1][0]\n",
    "        try:\n",
    "            word = lemmatizer.lemmatize(tk, pos=tag.lower())\n",
    "        except KeyError:\n",
    "            word = tk\n",
    "        output.append(word)\n",
    "    output = ' '.join(output)\n",
    "    return output\n",
    "\n",
    "df_prepro['tweet'] = df_prepro.tweet.apply(lemmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "\n",
    "english_word = set(words.words())\n",
    "\n",
    "def remove_unknown_word(entry):    \n",
    "    output = [w for w in word_tokenize(entry) if w.lower() in english_word]\n",
    "    output = ' '.join(output)\n",
    "    return output\n",
    "\n",
    "df_prepro['tweet'] = df_prepro.tweet.apply(remove_unknown_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_tweet = df_prepro['tweet'].str.len() > 33\n",
    "df_prepro = df_prepro[long_tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro = df_prepro.drop_duplicates(subset='tweet', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prepro.loc[:,'created_date'] = pd.to_datetime(df_prepro['created_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean sample dataset have 7277 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7277 entries, 0 to 38216\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype              \n",
      "---  ------        --------------  -----              \n",
      " 0   created_date  7277 non-null   datetime64[ns, UTC]\n",
      " 1   tweet         7277 non-null   object             \n",
      " 2   tweet_id      7277 non-null   int64              \n",
      " 3   sentiment     7277 non-null   float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(1), int64(1), object(1)\n",
      "memory usage: 284.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_prepro.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thu Mar 19 19:52:18 +0000 2020</td>\n",
       "      <td>lot would actually benefit take good look intr...</td>\n",
       "      <td>1240727821028405249</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thu Mar 19 19:52:16 +0000 2020</td>\n",
       "      <td>somewhere studio sing body dey kill person like</td>\n",
       "      <td>1240727810249043969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thu Mar 19 19:52:16 +0000 2020</td>\n",
       "      <td>appreciate move fight corona never fan hater j</td>\n",
       "      <td>1240727809300901888</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thu Mar 19 19:52:17 +0000 2020</td>\n",
       "      <td>everything fine world decide take premier leag...</td>\n",
       "      <td>1240727817555447808</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thu Mar 19 19:52:18 +0000 2020</td>\n",
       "      <td>here delete scene special corona virus</td>\n",
       "      <td>1240727819119968256</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Thu Mar 19 19:52:19 +0000 2020</td>\n",
       "      <td>seriously call stupid come country large numbe...</td>\n",
       "      <td>1240727822089351168</td>\n",
       "      <td>-0.263095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Thu Mar 19 19:52:20 +0000 2020</td>\n",
       "      <td>new twist classic greater good queen</td>\n",
       "      <td>1240727828989165571</td>\n",
       "      <td>0.375758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Thu Mar 19 19:52:20 +0000 2020</td>\n",
       "      <td>generation z want name folk love call everybody</td>\n",
       "      <td>1240727829668560903</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Thu Mar 19 19:52:20 +0000 2020</td>\n",
       "      <td>please team researcher look volunteer translat...</td>\n",
       "      <td>1240727828989202437</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Thu Mar 19 19:52:21 +0000 2020</td>\n",
       "      <td>generation z want name folk love call everybody</td>\n",
       "      <td>1240727830410952704</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_date  \\\n",
       "0    Thu Mar 19 19:52:18 +0000 2020   \n",
       "2    Thu Mar 19 19:52:16 +0000 2020   \n",
       "3    Thu Mar 19 19:52:16 +0000 2020   \n",
       "4    Thu Mar 19 19:52:17 +0000 2020   \n",
       "5    Thu Mar 19 19:52:18 +0000 2020   \n",
       "..                              ...   \n",
       "144  Thu Mar 19 19:52:19 +0000 2020   \n",
       "145  Thu Mar 19 19:52:20 +0000 2020   \n",
       "148  Thu Mar 19 19:52:20 +0000 2020   \n",
       "151  Thu Mar 19 19:52:20 +0000 2020   \n",
       "152  Thu Mar 19 19:52:21 +0000 2020   \n",
       "\n",
       "                                                 tweet             tweet_id  \\\n",
       "0    lot would actually benefit take good look intr...  1240727821028405249   \n",
       "2      somewhere studio sing body dey kill person like  1240727810249043969   \n",
       "3       appreciate move fight corona never fan hater j  1240727809300901888   \n",
       "4    everything fine world decide take premier leag...  1240727817555447808   \n",
       "5               here delete scene special corona virus  1240727819119968256   \n",
       "..                                                 ...                  ...   \n",
       "144  seriously call stupid come country large numbe...  1240727822089351168   \n",
       "145               new twist classic greater good queen  1240727828989165571   \n",
       "148    generation z want name folk love call everybody  1240727829668560903   \n",
       "151  please team researcher look volunteer translat...  1240727828989202437   \n",
       "152    generation z want name folk love call everybody  1240727830410952704   \n",
       "\n",
       "     sentiment  \n",
       "0     0.350000  \n",
       "2     0.000000  \n",
       "3     0.000000  \n",
       "4     0.416667  \n",
       "5     0.357143  \n",
       "..         ...  \n",
       "144  -0.263095  \n",
       "145   0.375758  \n",
       "148   0.500000  \n",
       "151   0.500000  \n",
       "152   0.500000  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prepro.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation of the data cleaning pipeline script\n",
    "'cd ..' is to make relative import work for jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simon\\OneDrive\\Desktop\\team5\\team5\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load preprocessed sample dataset and perform cleaning with data cleaning script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\simon\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from src.datapipeline.clean_data import clean_pipeline\n",
    "\n",
    "df_prepro = pd.read_csv('data/sample/corona_sample.csv', index_col = 0)\n",
    "\n",
    "df_clean = clean_pipeline(df_prepro, min_length=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset cleaned by the script also have 7277 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7277 entries, 0 to 38216\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype              \n",
      "---  ------        --------------  -----              \n",
      " 0   created_date  7277 non-null   datetime64[ns, UTC]\n",
      " 1   tweet         7277 non-null   object             \n",
      " 2   tweet_id      7277 non-null   int64              \n",
      " 3   sentiment     7277 non-null   float64            \n",
      "dtypes: datetime64[ns, UTC](1), float64(1), int64(1), object(1)\n",
      "memory usage: 284.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_date</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-19 19:52:18+00:00</td>\n",
       "      <td>lot would actually benefit take good look intr...</td>\n",
       "      <td>1240727821028405249</td>\n",
       "      <td>0.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-03-19 19:52:16+00:00</td>\n",
       "      <td>somewhere studio sing body dey kill person like</td>\n",
       "      <td>1240727810249043969</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-03-19 19:52:16+00:00</td>\n",
       "      <td>appreciate move fight corona never fan hater j</td>\n",
       "      <td>1240727809300901888</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-03-19 19:52:17+00:00</td>\n",
       "      <td>everything fine world decide take premier leag...</td>\n",
       "      <td>1240727817555447808</td>\n",
       "      <td>0.416667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-03-19 19:52:18+00:00</td>\n",
       "      <td>here delete scene special corona virus</td>\n",
       "      <td>1240727819119968256</td>\n",
       "      <td>0.357143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>2020-03-19 19:52:23+00:00</td>\n",
       "      <td>interest get name spiked ring protein surface ...</td>\n",
       "      <td>1240727839848181761</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>2020-03-19 19:52:22+00:00</td>\n",
       "      <td>share corona virus vaccine owe life</td>\n",
       "      <td>1240727836899430405</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>2020-03-19 19:52:22+00:00</td>\n",
       "      <td>inform say corona virus die degree guess turn ...</td>\n",
       "      <td>1240727835091841026</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2020-03-19 19:52:22+00:00</td>\n",
       "      <td>burnt as really think various conspiracy coron...</td>\n",
       "      <td>1240727836421492738</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>2020-03-19 19:52:22+00:00</td>\n",
       "      <td>get bit zombie movie try hide everyone else</td>\n",
       "      <td>1240727836463443976</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_date  \\\n",
       "0   2020-03-19 19:52:18+00:00   \n",
       "2   2020-03-19 19:52:16+00:00   \n",
       "3   2020-03-19 19:52:16+00:00   \n",
       "4   2020-03-19 19:52:17+00:00   \n",
       "5   2020-03-19 19:52:18+00:00   \n",
       "..                        ...   \n",
       "227 2020-03-19 19:52:23+00:00   \n",
       "229 2020-03-19 19:52:22+00:00   \n",
       "233 2020-03-19 19:52:22+00:00   \n",
       "235 2020-03-19 19:52:22+00:00   \n",
       "240 2020-03-19 19:52:22+00:00   \n",
       "\n",
       "                                                 tweet             tweet_id  \\\n",
       "0    lot would actually benefit take good look intr...  1240727821028405249   \n",
       "2      somewhere studio sing body dey kill person like  1240727810249043969   \n",
       "3       appreciate move fight corona never fan hater j  1240727809300901888   \n",
       "4    everything fine world decide take premier leag...  1240727817555447808   \n",
       "5               here delete scene special corona virus  1240727819119968256   \n",
       "..                                                 ...                  ...   \n",
       "227  interest get name spiked ring protein surface ...  1240727839848181761   \n",
       "229                share corona virus vaccine owe life  1240727836899430405   \n",
       "233  inform say corona virus die degree guess turn ...  1240727835091841026   \n",
       "235  burnt as really think various conspiracy coron...  1240727836421492738   \n",
       "240        get bit zombie movie try hide everyone else  1240727836463443976   \n",
       "\n",
       "     sentiment  \n",
       "0     0.350000  \n",
       "2     0.000000  \n",
       "3     0.000000  \n",
       "4     0.416667  \n",
       "5     0.357143  \n",
       "..         ...  \n",
       "227   0.500000  \n",
       "229   0.000000  \n",
       "233   0.000000  \n",
       "235   0.100000  \n",
       "240   0.000000  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
